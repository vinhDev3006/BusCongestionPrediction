{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a9cf9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "\n",
    "\"\"\"\n",
    "    LOAD DATA\n",
    "\"\"\"\n",
    "FILE_PATH = os.path.join(\"dataset\", \"main_data.csv\")\n",
    "df = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1eab86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    CLEAN DATA\n",
    "\"\"\"\n",
    "col = ['trip_id', 'start_stop_id', 'arrival_time', 'stop_sequence_x', 'stop_lat', 'stop_lon', 'route_id',\n",
    "       'direction_id', 'speed_kmh', 'segment_max_speed_kmh', 'runtime_sec', 'end_stop_id', 'distance_m']\n",
    "df = df[col]\n",
    "\n",
    "# df['arrival_time'] = pd.to_datetime(df['arrival_time'].dt.strftime('%H:%M:%S'))\n",
    "df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c330e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinhn\\AppData\\Local\\Temp\\ipykernel_9664\\1649409532.py:11: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  df['congestion_level'] = pd.np.select(conditions, congestion_levels, default=4)\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by=['direction_id', 'trip_id'], ascending=[True, True])\n",
    "df['decisive_speed'] = df.apply(lambda row, weight=random.uniform(0.8, 0.9): (\n",
    "            row['speed_kmh'] * weight + row['segment_max_speed_kmh'] * (1 - weight)), axis=1)\n",
    "conditions = [\n",
    "    (df['decisive_speed'] > 40),\n",
    "    (df['decisive_speed'] > 30),\n",
    "    (df['decisive_speed'] > 20),\n",
    "    (df['decisive_speed'] > 15)\n",
    "]\n",
    "congestion_levels = [0, 1, 2, 3]\n",
    "df['congestion_level'] = pd.np.select(conditions, congestion_levels, default=4)\n",
    "df = df.drop(['decisive_speed'], axis=1)\n",
    "df = df.drop_duplicates(subset=['trip_id', 'start_stop_id', 'arrival_time', 'stop_sequence_x', 'stop_lat', 'stop_lon', 'route_id',\n",
    "       'direction_id', 'speed_kmh', 'segment_max_speed_kmh', 'runtime_sec', 'end_stop_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14d8babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'trip_id' and 'direction_id' columns\n",
    "df_sorted = df.sort_values(['trip_id', 'direction_id'])\n",
    "\n",
    "# Group the sorted DataFrame by 'trip_id' and 'direction_id'\n",
    "grouped = df_sorted.groupby(['trip_id', 'direction_id'])\n",
    "\n",
    "# Create a list to store the modified DataFrames\n",
    "subdatasets = []\n",
    "\n",
    "for key, subdataset in grouped:\n",
    "    # Sort the subdataset by 'arrival_time' to ensure the correct order\n",
    "    subdataset = subdataset.sort_values('arrival_time')\n",
    "\n",
    "    # Use .shift() to get the next latitude and longitude values\n",
    "    subdataset['next_lat'] = subdataset['stop_lat'].shift(-1)\n",
    "    subdataset['next_lon'] = subdataset['stop_lon'].shift(-1)\n",
    "\n",
    "    # Replace 'NaN' in the last row of 'next_lat' and 'next_lon' with the current values\n",
    "    subdataset['next_lat'].fillna(subdataset['stop_lat'].iloc[-1], inplace=True)\n",
    "    subdataset['next_lon'].fillna(subdataset['stop_lon'].iloc[-1], inplace=True)\n",
    "\n",
    "    # Append the modified subdataset to the list\n",
    "    subdatasets.append(subdataset)\n",
    "\n",
    "# Concatenate the modified subdatasets into one dataset\n",
    "merged_dataset = pd.concat(subdatasets, ignore_index=True)\n",
    "\n",
    "# Reset the index of the merged dataset\n",
    "merged_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the merged dataset\n",
    "df = merged_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9741096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['trip_id', 'direction_id'])\n",
    "\n",
    "sub_datasets = {group_name: group_df for group_name, group_df in grouped}\n",
    "dataset = []\n",
    "# Access the sub-datasets as needed\n",
    "for key, sub_dataset in sub_datasets.items():\n",
    "    # print(f\"Sub-dataset for {key}:\")\n",
    "    # Reset the index of the sub-dataset\n",
    "    sub_dataset = sub_dataset.reset_index(drop=True)\n",
    "\n",
    "    calculated_arrival_times = []\n",
    "\n",
    "    for idx in range(len(sub_dataset)):\n",
    "        if idx < 1:\n",
    "            # For the first two rows, use the original 'arrival_time' values\n",
    "            calculated_arrival_times.append(sub_dataset['arrival_time'].iloc[idx])\n",
    "        else:\n",
    "            # For subsequent rows, calculate the 'arrival_time' based on the two previous rows\n",
    "            previous_arrival_time = calculated_arrival_times[idx - 1]\n",
    "            previous_runtime = sub_dataset['runtime_sec'].iloc[idx - 1]\n",
    "            new_arrival_time = previous_arrival_time + pd.to_timedelta(previous_runtime, unit='s')\n",
    "            calculated_arrival_times.append(new_arrival_time)\n",
    "\n",
    "    # Replace the original 'arrival_time' column with the calculated values\n",
    "    sub_dataset['arrival_time'] = calculated_arrival_times\n",
    "    dataset.append(sub_dataset)\n",
    "# Concatenate all sub-datasets into one dataset\n",
    "merged_dataset = pd.concat(dataset, ignore_index=True)\n",
    "\n",
    "# Reset the index of the merged dataset\n",
    "merged_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6bc92cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a27b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
